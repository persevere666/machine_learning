{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a31decab-3ace-48d1-9802-c17f4c77a5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yangyw/miniconda3/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = 30, 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ccc0c88-1f83-4922-85b5-05ff8e4d7728",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, optimizers, datasets, Sequential, metrics  # 导入TF子库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d226cf02-92bc-4d1f-829f-6b9e48397a9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_5 (Conv2D)           (32, 54, 54, 96)          34944     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (32, 26, 26, 96)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (32, 22, 22, 256)         614656    \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPoolin  (32, 10, 10, 256)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (32, 8, 8, 384)           885120    \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (32, 6, 6, 384)           1327488   \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (32, 4, 4, 256)           884992    \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPoolin  (32, 1, 1, 256)           0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (32, 256)                 0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (32, 2048)                526336    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (32, 2048)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (32, 2048)                4196352   \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (32, 2048)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (32, 1000)                2049000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10518888 (40.13 MB)\n",
      "Trainable params: 10518888 (40.13 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 2.网络搭建\n",
    "network = Sequential([\n",
    "    # 第一层\n",
    "    layers.Conv2D(96, kernel_size=11, strides=4, activation='relu'),  # 55*55*48\n",
    "    layers.MaxPooling2D(pool_size=3, strides=2),  # 27*27*48\n",
    "    # 第二层\n",
    "    layers.Conv2D(256, kernel_size=5, strides=1, activation='relu'),  # 27*27*128\n",
    "    layers.MaxPooling2D(pool_size=3, strides=2),  # 13*13*128\n",
    "    # 第三层\n",
    "    layers.Conv2D(384, kernel_size=3, strides=1, activation='relu'),  # 13*13*192\n",
    "    # 第四层\n",
    "    layers.Conv2D(384, kernel_size=3, strides=1, activation='relu'),  # 13*13*192\n",
    "    # 第五层\n",
    "    layers.Conv2D(256, kernel_size=3, strides=1, activation='relu'),  # 13*13*128\n",
    "    layers.MaxPooling2D(pool_size=3, strides=2),  # 6*6*128\n",
    "    layers.Flatten(),  # 6*6*128=4608\n",
    "    # 第六层\n",
    "    layers.Dense(2048, activation='relu'),\n",
    "    layers.Dropout(rate=0.5),\n",
    "    # 第七层\n",
    "    layers.Dense(2048, activation='relu'),\n",
    "    layers.Dropout(rate=0.5),\n",
    "    # 第八层（输出层）\n",
    "    layers.Dense(1000)\n",
    "])\n",
    "network.build(input_shape=(32, 224, 224, 3))  # 设置输入格式\n",
    "network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "830fcf18-3c9f-4095-b79f-27a449c8235b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/animikhaich/AlexNet-Tensorflow\n",
    "class ImageNetDataLoader:\n",
    "    def __init__(\n",
    "        self,\n",
    "        source_data_dir: str,\n",
    "        dest_data_dir: str,\n",
    "        split: str = \"train\",\n",
    "        image_dims: tuple = (224, 224),\n",
    "        num_classes=1000\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        __init__\n",
    "        - Instance Variable Initialization\n",
    "        - Download and Set Up Dataset (One Time Operation)\n",
    "        - Use TFDS to Load and convert the ImageNet Dataset\n",
    "        \n",
    "        Args:\n",
    "            source_data_dir (str): Path to Downloaded tar files\n",
    "            dest_data_dir (str): Path to the location where the dataset will be unpacked\n",
    "            spliit (str): Split to load as. Eg. train, test, train[:80%]. Defaults to \"train\"\n",
    "            image_dims (tuple, optional): Image Dimensions (width & height). Defaults to (224, 224).\n",
    "            num_classes (int): Number of Classes contained in this dataset. Defaults to 1000\n",
    "        \"\"\"\n",
    "        \n",
    "        # Constants\n",
    "        self.NUM_CLASSES=num_classes\n",
    "        self.BATCH_SIZE = None\n",
    "        self.NUM_CHANNELS = 3\n",
    "        self.LABELS = []\n",
    "        self.LABELMAP = {}\n",
    "        self.AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "        self.WIDTH, self.HEIGHT = image_dims    \n",
    "        \n",
    "        # Download Config\n",
    "        download_config = tfds.download.DownloadConfig(\n",
    "            extract_dir=os.path.join(dest_data_dir, 'extracted'),\n",
    "            manual_dir=source_data_dir\n",
    "        )\n",
    "\n",
    "        download_and_prepare_kwargs = {\n",
    "            'download_dir': os.path.join(dest_data_dir, 'downloaded'),\n",
    "            'download_config': download_config,\n",
    "        }\n",
    "        \n",
    "        # TFDS Data Loader (This step also performs dataset conversion to TFRecord)\n",
    "        self.dataset, self.info = tfds.load(\n",
    "            'imagenet2012', \n",
    "            data_dir=os.path.join(dest_data_dir, 'data'),         \n",
    "            split=split, \n",
    "            shuffle_files=True, \n",
    "            download=True, \n",
    "            as_supervised=True,\n",
    "            with_info=True,\n",
    "            download_and_prepare_kwargs=download_and_prepare_kwargs\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def preprocess_image(self, image, label):\n",
    "        \"\"\"\n",
    "        preprocess_image\n",
    "        \n",
    "        Process the image and label to perform the following operations:\n",
    "        - Min Max Scale the Image (Divide by 255)\n",
    "        - Convert the numerical values of the lables to One Hot Encoded Format\n",
    "        - Resize the image to 224, 224\n",
    "        \n",
    "        Args:\n",
    "            image (Image Tensor): Raw Image\n",
    "            label (Tensor): Numeric Labels 1, 2, 3, ...\n",
    "        Returns:\n",
    "            tuple: Scaled Image, One-Hot Encoded Label\n",
    "        \"\"\"\n",
    "        image = tf.cast(image, tf.uint8)\n",
    "        image = tf.image.resize(image, [self.HEIGHT, self.WIDTH])\n",
    "        image = image / tf.math.reduce_max(image)\n",
    "        label = tf.one_hot(indices=label, depth=self.NUM_CLASSES)\n",
    "        return image, label\n",
    "    \n",
    "    @tf.function\n",
    "    def augment_batch(self, image, label) -> tuple:\n",
    "        \"\"\"\n",
    "        augment_batch\n",
    "        Image Augmentation for Training:\n",
    "        - Random Contrast\n",
    "        - Random Brightness\n",
    "        - Random Hue (Color)\n",
    "        - Random Saturation\n",
    "        - Random Horizontal Flip\n",
    "        - Random Reduction in Image Quality\n",
    "        - Random Crop\n",
    "        Args:\n",
    "            image (Tensor Image): Raw Image\n",
    "            label (Tensor): Numeric Labels 1, 2, 3, ...\n",
    "        Returns:\n",
    "            tuple: Augmented Image, Numeric Labels 1, 2, 3, ...\n",
    "        \"\"\"\n",
    "        if tf.random.normal([1]) < 0:\n",
    "            image = tf.image.random_contrast(image, 0.2, 0.9)\n",
    "        if tf.random.normal([1]) < 0:\n",
    "            image = tf.image.random_brightness(image, 0.2)\n",
    "        if self.NUM_CHANNELS == 3 and tf.random.normal([1]) < 0:\n",
    "            image = tf.image.random_hue(image, 0.3)\n",
    "        if self.NUM_CHANNELS == 3 and tf.random.normal([1]) < 0:\n",
    "            image = tf.image.random_saturation(image, 0, 15)\n",
    "        \n",
    "        image = tf.image.random_flip_left_right(image)\n",
    "        image = tf.image.random_jpeg_quality(image, 10, 100)\n",
    "\n",
    "        return image, label\n",
    "    \n",
    "    def get_dataset_size(self) -> int:\n",
    "        \"\"\"\n",
    "        get_dataset_size\n",
    "        Get the Dataset Size (Number of Images)\n",
    "        Returns:\n",
    "            int: Total Number of images in Dataset\n",
    "        \"\"\"\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def get_num_steps(self) -> int:\n",
    "        \"\"\"\n",
    "        get_num_steps\n",
    "        Get the Number of Steps Required per Batch for Training\n",
    "        Raises:\n",
    "            AssertionError: Dataset Generator needs to be Initialized First\n",
    "        Returns:\n",
    "            int: Number of Steps Required for Training Per Batch\n",
    "        \"\"\"\n",
    "        if self.BATCH_SIZE is None:\n",
    "            raise AssertionError(\n",
    "                f\"Batch Size is not Initialized. Call this method only after calling: {self.dataset_generator}\"\n",
    "            )\n",
    "        num_steps = self.get_dataset_size() // self.BATCH_SIZE + 1\n",
    "        return num_steps\n",
    "    \n",
    "    def dataset_generator(self, batch_size=32, augment=False):\n",
    "        \"\"\"\n",
    "        dataset_generator\n",
    "        Create the Data Loader Pipeline and Return a Generator to Generate Datsets\n",
    "        Args:\n",
    "            batch_size (int, optional): Batch Size. Defaults to 32.\n",
    "            augment (bool, optional): Enable/Disable Augmentation. Defaults to False.\n",
    "        Returns:\n",
    "            Tf.Data Generator: Dataset Generator\n",
    "        \"\"\"\n",
    "        self.BATCH_SIZE = batch_size\n",
    "\n",
    "        dataset = self.dataset.apply(tf.data.experimental.ignore_errors())\n",
    "\n",
    "        dataset = dataset.shuffle(batch_size * 10)\n",
    "        dataset = dataset.repeat()\n",
    "               \n",
    "        if augment:\n",
    "            dataset = dataset.map(self.augment_batch, num_parallel_calls=self.AUTOTUNE)\n",
    "        \n",
    "        dataset = dataset.map(self.preprocess_image, num_parallel_calls=self.AUTOTUNE)\n",
    "        \n",
    "        dataset = dataset.batch(batch_size)\n",
    "        dataset = dataset.prefetch(buffer_size=self.AUTOTUNE)\n",
    "\n",
    "        return dataset\n",
    "    \n",
    "    def visualize_batch(self, augment=True) -> None:\n",
    "        \"\"\"\n",
    "        visualize_batch\n",
    "        Dataset Sample Visualization\n",
    "        - Supports Augmentation\n",
    "        - Automatically Adjusts for Grayscale Images\n",
    "        Args:\n",
    "            augment (bool, optional): Enable/Disable Augmentation. Defaults to True.\n",
    "        \"\"\"\n",
    "        if self.NUM_CHANNELS == 1:\n",
    "            cmap = \"gray\"\n",
    "        else:\n",
    "            cmap = \"viridis\"\n",
    "\n",
    "        dataset = self.dataset_generator(batch_size=36, augment=augment)\n",
    "        image_batch, label_batch = next(iter(dataset))\n",
    "        image_batch, label_batch = (\n",
    "            image_batch.numpy(),\n",
    "            label_batch.numpy(),\n",
    "        )\n",
    "\n",
    "        for n in range(len(image_batch)):\n",
    "            ax = plt.subplot(6, 6, n + 1)\n",
    "            plt.imshow(image_batch[n], cmap=cmap)\n",
    "            plt.title(np.argmax(label_batch[n]))\n",
    "            plt.axis(\"off\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3c8158a-a812-4a37-93fc-e12c84892606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " alexnet_input (InputLayer)  [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 56, 56, 96)        34944     \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 56, 56, 96)        384       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " re_lu (ReLU)                (None, 56, 56, 96)        0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 27, 27, 96)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 27, 27, 256)       614656    \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 27, 27, 256)       1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_1 (ReLU)              (None, 27, 27, 256)       0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 13, 13, 256)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 13, 13, 384)       885120    \n",
      "                                                                 \n",
      " re_lu_2 (ReLU)              (None, 13, 13, 384)       0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 13, 13, 384)       1327488   \n",
      "                                                                 \n",
      " re_lu_3 (ReLU)              (None, 13, 13, 384)       0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 13, 13, 256)       884992    \n",
      "                                                                 \n",
      " re_lu_4 (ReLU)              (None, 13, 13, 256)       0         \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 6, 6, 256)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 9216)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4096)              37752832  \n",
      "                                                                 \n",
      " re_lu_5 (ReLU)              (None, 4096)              0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4096)              16781312  \n",
      "                                                                 \n",
      " re_lu_6 (ReLU)              (None, 4096)              0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1000)              4097000   \n",
      "                                                                 \n",
      " alexnet_output (Softmax)    (None, 1000)              0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 62379752 (237.96 MB)\n",
      "Trainable params: 62379048 (237.96 MB)\n",
      "Non-trainable params: 704 (2.75 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/animikhaich/AlexNet-Tensorflow/blob/main/AlexNet_Trainer.ipynb\n",
    "\n",
    "inputs = tf.keras.Input(shape=(224, 224, 3), name=\"alexnet_input\")\n",
    "\n",
    "# Layer 1 - Convolutions\n",
    "l1 = tf.keras.layers.Conv2D(filters=96, kernel_size=11, strides=4, padding=\"same\")(inputs)\n",
    "l1 = tf.keras.layers.BatchNormalization()(l1)\n",
    "l1 = tf.keras.layers.ReLU()(l1)\n",
    "l1 = tf.keras.layers.MaxPooling2D(pool_size=3, strides=2)(l1)\n",
    "\n",
    "# Layer 2 - Convolutions\n",
    "l2 = tf.keras.layers.Conv2D(filters=256, kernel_size=5, strides=1, padding=\"same\")(l1)\n",
    "l2 = tf.keras.layers.BatchNormalization()(l2)\n",
    "l2 = tf.keras.layers.ReLU()(l2)\n",
    "l2 = tf.keras.layers.MaxPooling2D(pool_size=3, strides=2)(l2)\n",
    "\n",
    "# Layer 3 - Convolutions\n",
    "l3 = tf.keras.layers.Conv2D(filters=384, kernel_size=3, strides=1, padding=\"same\")(l2)\n",
    "l3 = tf.keras.layers.ReLU()(l3)\n",
    "\n",
    "# Layer 4 - Convolutions\n",
    "l4 = tf.keras.layers.Conv2D(filters=384, kernel_size=3, strides=1, padding=\"same\")(l3)\n",
    "l4 = tf.keras.layers.ReLU()(l4)\n",
    "\n",
    "# Layer 5 - Convolutions\n",
    "l5 = tf.keras.layers.Conv2D(filters=256, kernel_size=3, strides=1, padding=\"same\")(l4)\n",
    "l5 = tf.keras.layers.ReLU()(l5)\n",
    "l5 = tf.keras.layers.MaxPooling2D(pool_size=3, strides=2)(l5)\n",
    "\n",
    "# Layer 6 - Dense\n",
    "l6_pre = tf.keras.layers.Flatten()(l5)\n",
    "\n",
    "l6 = tf.keras.layers.Dense(units=4096)(l6_pre)\n",
    "l6 = tf.keras.layers.ReLU()(l6)\n",
    "l6 = tf.keras.layers.Dropout(rate=0.5)(l6)\n",
    "\n",
    "# Layer 7 - Dense\n",
    "l7 = tf.keras.layers.Dense(units=4096)(l6)\n",
    "l7 = tf.keras.layers.ReLU()(l7)\n",
    "l7 = tf.keras.layers.Dropout(rate=0.5)(l7)\n",
    "\n",
    "# Layer 8 - Dense\n",
    "l8 = tf.keras.layers.Dense(units=1000)(l7)\n",
    "l8 = tf.keras.layers.Softmax(dtype=tf.float32, name=\"alexnet_output\")(l8)\n",
    "\n",
    "alexnet = tf.keras.models.Model(inputs=inputs, outputs=l8)\n",
    "alexnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f38278f-1e35-4813-917f-60876fc0d7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(alexnet, show_layer_names=False, show_shapes=True, show_dtype=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4e55f41-a340-4beb-9b09-03915fafc617",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-20 14:48:37.420112: I tensorflow/tsl/profiler/lib/profiler_session.cc:104] Profiler session initializing.\n",
      "2023-09-20 14:48:37.420142: I tensorflow/tsl/profiler/lib/profiler_session.cc:119] Profiler session started.\n",
      "2023-09-20 14:48:37.422241: I tensorflow/tsl/profiler/lib/profiler_session.cc:131] Profiler session tear down.\n"
     ]
    }
   ],
   "source": [
    "# Callbacks\n",
    "early_stop_cb = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_categorical_accuracy\",\n",
    "    min_delta=0,\n",
    "    patience=10,\n",
    "    verbose=1,\n",
    "    mode=\"auto\",\n",
    "    baseline=None,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "model_ckpt_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"weights/alexnet.{epoch:02d}-{val_categorical_accuracy:.2f}-{val_loss:.2f}.h5\",\n",
    "    monitor=\"val_categorical_accuracy\",\n",
    "    verbose=0,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    mode=\"auto\",\n",
    "    save_freq=\"epoch\",\n",
    ")\n",
    "reduce_lr_cb = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_categorical_accuracy\",\n",
    "    factor=0.1,\n",
    "    patience=2,\n",
    "    verbose=0,\n",
    "    mode=\"auto\",\n",
    "    min_delta=0.0001,\n",
    "    cooldown=0,\n",
    "    min_lr=10e-8,\n",
    ")\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=\"tb_logs/\",\n",
    "    histogram_freq=2,\n",
    "    write_graph=True,\n",
    "    write_images=False,\n",
    "    update_freq=\"epoch\",\n",
    "    profile_batch=2,\n",
    "    embeddings_freq=2,\n",
    "    embeddings_metadata=None,\n",
    ")\n",
    "callbacks = [early_stop_cb, model_ckpt_cb, reduce_lr_cb, tensorboard_cb]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ddc83b93-0d32-4915-ae33-4c706a8c07d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "metrics = [\n",
    "    tf.keras.metrics.CategoricalAccuracy(),\n",
    "    tf.keras.metrics.FalseNegatives(),\n",
    "    tf.keras.metrics.FalsePositives(),\n",
    "    tf.keras.metrics.Precision(),\n",
    "    tf.keras.metrics.Recall(),\n",
    "    tfa.metrics.F1Score(num_classes=1000)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc655fb9-c4a9-4613-9a89-c580af281037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING\n",
      "The dtype policy mixed_float16 may run slowly because this machine does not have a GPU. Only Nvidia GPUs with compute capability of at least 7.0 run quickly with mixed_float16.\n",
      "If you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once\n"
     ]
    }
   ],
   "source": [
    "# Enable Mixed Precision Training for Supported GPUs to utilize the optimized Tensor Cores for Matrix Operations.\n",
    "# Mixed Precision\n",
    "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f41b9724-2373-4700-b1b2-b6b2c7423e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-20 15:18:19.912804: W tensorflow/tsl/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata.google.internal\".\n",
      "2023-09-20 15:19:20.921261: E tensorflow/tsl/platform/cloud/curl_http_request.cc:610] The transmission  of request 0x7f8db26ef270 (URI: https://www.googleapis.com/storage/v1/b/tfds-data/o/dataset_info%2Fimagenet2012%2F5.1.0?fields=size%2Cgeneration%2Cupdated) has been stuck at 0 of 0 bytes for 61 seconds and will be aborted. CURL timing information: lookup time: 0.012695 (No error), connect time: 0 (No error), pre-transfer time: 0 (No error), start-transfer time: 0 (No error)\n",
      "2023-09-20 15:20:23.718327: E tensorflow/tsl/platform/cloud/curl_http_request.cc:610] The transmission  of request 0x7f8d93c1d400 (URI: https://www.googleapis.com/storage/v1/b/tfds-data/o/dataset_info%2Fimagenet2012%2F5.1.0?fields=size%2Cgeneration%2Cupdated) has been stuck at 0 of 0 bytes for 61 seconds and will be aborted. CURL timing information: lookup time: 0.002798 (No error), connect time: 0 (No error), pre-transfer time: 0 (No error), start-transfer time: 0 (No error)\n",
      "2023-09-20 15:21:27.398021: E tensorflow/tsl/platform/cloud/curl_http_request.cc:610] The transmission  of request 0x7f8d93c1d400 (URI: https://www.googleapis.com/storage/v1/b/tfds-data/o/dataset_info%2Fimagenet2012%2F5.1.0?fields=size%2Cgeneration%2Cupdated) has been stuck at 0 of 0 bytes for 61 seconds and will be aborted. CURL timing information: lookup time: 0.001871 (No error), connect time: 0 (No error), pre-transfer time: 0 (No error), start-transfer time: 0 (No error)\n",
      "2023-09-20 15:22:32.637730: E tensorflow/tsl/platform/cloud/curl_http_request.cc:610] The transmission  of request 0x7f8d83fbc470 (URI: https://www.googleapis.com/storage/v1/b/tfds-data/o/dataset_info%2Fimagenet2012%2F5.1.0?fields=size%2Cgeneration%2Cupdated) has been stuck at 0 of 0 bytes for 61 seconds and will be aborted. CURL timing information: lookup time: 0.021358 (No error), connect time: 0 (No error), pre-transfer time: 0 (No error), start-transfer time: 0 (No error)\n",
      "2023-09-20 15:23:41.677581: E tensorflow/tsl/platform/cloud/curl_http_request.cc:610] The transmission  of request 0x7f8d93c1d400 (URI: https://www.googleapis.com/storage/v1/b/tfds-data/o/dataset_info%2Fimagenet2012%2F5.1.0?fields=size%2Cgeneration%2Cupdated) has been stuck at 0 of 0 bytes for 61 seconds and will be aborted. CURL timing information: lookup time: 0.002657 (No error), connect time: 0 (No error), pre-transfer time: 0 (No error), start-transfer time: 0 (No error)\n",
      "2023-09-20 15:24:59.521373: E tensorflow/tsl/platform/cloud/curl_http_request.cc:610] The transmission  of request 0x7f8d83fbc470 (URI: https://www.googleapis.com/storage/v1/b/tfds-data/o/dataset_info%2Fimagenet2012%2F5.1.0?fields=size%2Cgeneration%2Cupdated) has been stuck at 0 of 0 bytes for 61 seconds and will be aborted. CURL timing information: lookup time: 0.002617 (No error), connect time: 0 (No error), pre-transfer time: 0 (No error), start-transfer time: 0 (No error)\n",
      "2023-09-20 15:26:33.087443: E tensorflow/tsl/platform/cloud/curl_http_request.cc:610] The transmission  of request 0x7f8db23ee7f0 (URI: https://www.googleapis.com/storage/v1/b/tfds-data/o/dataset_info%2Fimagenet2012%2F5.1.0?fields=size%2Cgeneration%2Cupdated) has been stuck at 0 of 0 bytes for 61 seconds and will be aborted. CURL timing information: lookup time: 0.002063 (No error), connect time: 0 (No error), pre-transfer time: 0 (No error), start-transfer time: 0 (No error)\n",
      "2023-09-20 15:28:06.602233: E tensorflow/tsl/platform/cloud/curl_http_request.cc:610] The transmission  of request 0x7f8d83fbc470 (URI: https://www.googleapis.com/storage/v1/b/tfds-data/o/dataset_info%2Fimagenet2012%2F5.1.0?fields=size%2Cgeneration%2Cupdated) has been stuck at 0 of 0 bytes for 61 seconds and will be aborted. CURL timing information: lookup time: 0.002661 (No error), connect time: 0 (No error), pre-transfer time: 0 (No error), start-transfer time: 0 (No error)\n",
      "2023-09-20 15:29:39.859950: E tensorflow/tsl/platform/cloud/curl_http_request.cc:610] The transmission  of request 0x7f8d93c1d400 (URI: https://www.googleapis.com/storage/v1/b/tfds-data/o/dataset_info%2Fimagenet2012%2F5.1.0?fields=size%2Cgeneration%2Cupdated) has been stuck at 0 of 0 bytes for 61 seconds and will be aborted. CURL timing information: lookup time: 0.012484 (No error), connect time: 0 (No error), pre-transfer time: 0 (No error), start-transfer time: 0 (No error)\n",
      "2023-09-20 15:31:13.705023: E tensorflow/tsl/platform/cloud/curl_http_request.cc:610] The transmission  of request 0x7f8d93c1d400 (URI: https://www.googleapis.com/storage/v1/b/tfds-data/o/dataset_info%2Fimagenet2012%2F5.1.0?fields=size%2Cgeneration%2Cupdated) has been stuck at 0 of 0 bytes for 61 seconds and will be aborted. CURL timing information: lookup time: 0.002434 (No error), connect time: 0 (No error), pre-transfer time: 0 (No error), start-transfer time: 0 (No error)\n",
      "2023-09-20 15:32:47.468044: E tensorflow/tsl/platform/cloud/curl_http_request.cc:610] The transmission  of request 0x7f8db23ee7f0 (URI: https://www.googleapis.com/storage/v1/b/tfds-data/o/dataset_info%2Fimagenet2012%2F5.1.0?fields=size%2Cgeneration%2Cupdated) has been stuck at 0 of 0 bytes for 61 seconds and will be aborted. CURL timing information: lookup time: 0.007645 (No error), connect time: 0 (No error), pre-transfer time: 0 (No error), start-transfer time: 0 (No error)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to construct dataset imagenet2012: pybind11::error_already_set: MISMATCH of original and normalized active exception types: ORIGINAL AbortedError REPLACED BY KeyboardInterrupt: <EMPTY MESSAGE>\n\nAt:\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/tensorflow/python/framework/errors_impl.py(412): __init__\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/tensorflow/python/lib/io/file_io.py(290): file_exists_v2\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/etils/epath/backend.py(225): exists\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/etils/epath/gpath.py(144): exists\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/tensorflow_datasets/core/utils/gcs_utils.py(76): exists\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/tensorflow_datasets/core/utils/gcs_utils.py(93): gcs_dataset_info_path\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/tensorflow_datasets/core/utils/gcs_utils.py(100): gcs_dataset_info_files\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/tensorflow_datasets/core/dataset_info.py(784): initialize_from_bucket\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/tensorflow_datasets/core/dataset_builder.py(287): __init__\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/tensorflow_datasets/core/logging/__init__.py(286): decorator\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/tensorflow_datasets/core/dataset_builder.py(1319): __init__\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/tensorflow_datasets/core/logging/__init__.py(286): decorator\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/tensorflow_datasets/core/load.py(212): builder\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/tensorflow_datasets/core/logging/__init__.py(166): __call__\n  /Users/yangyw/miniconda3/lib/python3.10/contextlib.py(79): inner\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/tensorflow_datasets/core/load.py(488): _fetch_builder\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/tensorflow_datasets/core/load.py(633): load\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/tensorflow_datasets/core/logging/__init__.py(166): __call__\n  /var/folders/pr/f8n46jm50p10n21j0n65__400000gn/T/ipykernel_7677/388126983.py(45): __init__\n  /var/folders/pr/f8n46jm50p10n21j0n65__400000gn/T/ipykernel_7677/3716585327.py(6): <module>\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py(3526): run_code\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py(3466): run_ast_nodes\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py(3284): run_cell_async\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/IPython/core/async_helpers.py(129): _pseudo_sync_runner\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py(3079): _run_cell\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py(3024): run_cell\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/ipykernel/zmqshell.py(546): run_cell\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/ipykernel/ipkernel.py(422): do_execute\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/ipykernel/kernelbase.py(740): execute_request\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/ipykernel/kernelbase.py(412): dispatch_shell\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/ipykernel/kernelbase.py(505): process_one\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/ipykernel/kernelbase.py(516): dispatch_queue\n  /Users/yangyw/miniconda3/lib/python3.10/asyncio/events.py(80): _run\n  /Users/yangyw/miniconda3/lib/python3.10/asyncio/base_events.py(1899): _run_once\n  /Users/yangyw/miniconda3/lib/python3.10/asyncio/base_events.py(603): run_forever\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/tornado/platform/asyncio.py(215): start\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/ipykernel/kernelapp.py(736): start\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/traitlets/config/application.py(1043): launch_instance\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/ipykernel_launcher.py(17): <module>\n  /Users/yangyw/miniconda3/lib/python3.10/runpy.py(86): _run_code\n  /Users/yangyw/miniconda3/lib/python3.10/runpy.py(196): _run_module_as_main\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m BATCH_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Init Data Loaders\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m train_data_loader \u001b[38;5;241m=\u001b[39m \u001b[43mImageNetDataLoader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_data_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/mnt/data/pycodes/Dataset/imagenet2012\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdest_data_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/ani/Documents/datasets/imagenet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_dims\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m val_data_loader \u001b[38;5;241m=\u001b[39m ImageNetDataLoader(\n\u001b[1;32m     14\u001b[0m         source_data_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mnt/data/pycodes/Dataset/imagenet2012\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     15\u001b[0m         dest_data_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/ani/Documents/datasets/imagenet\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     16\u001b[0m         split \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     17\u001b[0m         image_dims \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m),\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     20\u001b[0m train_generator \u001b[38;5;241m=\u001b[39m train_data_loader\u001b[38;5;241m.\u001b[39mdataset_generator(batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, augment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[21], line 45\u001b[0m, in \u001b[0;36mImageNetDataLoader.__init__\u001b[0;34m(self, source_data_dir, dest_data_dir, split, image_dims, num_classes)\u001b[0m\n\u001b[1;32m     39\u001b[0m download_and_prepare_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdownload_dir\u001b[39m\u001b[38;5;124m'\u001b[39m: os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dest_data_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdownloaded\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdownload_config\u001b[39m\u001b[38;5;124m'\u001b[39m: download_config,\n\u001b[1;32m     42\u001b[0m }\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# TFDS Data Loader (This step also performs dataset conversion to TFRecord)\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo \u001b[38;5;241m=\u001b[39m \u001b[43mtfds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimagenet2012\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdest_data_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mas_supervised\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/tensorflow_datasets/core/logging/__init__.py:166\u001b[0m, in \u001b[0;36m_FunctionDecorator.__call__\u001b[0;34m(self, function, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_call()\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    168\u001b[0m   metadata\u001b[38;5;241m.\u001b[39mmark_error()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/tensorflow_datasets/core/load.py:633\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;129m@tfds_logging\u001b[39m\u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m    503\u001b[0m     name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    518\u001b[0m ):\n\u001b[1;32m    519\u001b[0m   \u001b[38;5;66;03m# pylint: disable=line-too-long\u001b[39;00m\n\u001b[1;32m    520\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Loads the named dataset into a `tf.data.Dataset`.\u001b[39;00m\n\u001b[1;32m    521\u001b[0m \n\u001b[1;32m    522\u001b[0m \u001b[38;5;124;03m  `tfds.load` is a convenience method that:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;124;03m      Split-specific information is available in `ds_info.splits`.\u001b[39;00m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m   dbuilder \u001b[38;5;241m=\u001b[39m \u001b[43m_fetch_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbuilder_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtry_gcs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    639\u001b[0m   _download_and_prepare_builder(dbuilder, download, download_and_prepare_kwargs)\n\u001b[1;32m    641\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m as_dataset_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/tensorflow_datasets/core/load.py:488\u001b[0m, in \u001b[0;36m_fetch_builder\u001b[0;34m(name, data_dir, builder_kwargs, try_gcs)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m builder_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    487\u001b[0m   builder_kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 488\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuilder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtry_gcs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtry_gcs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbuilder_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/tensorflow_datasets/core/logging/__init__.py:166\u001b[0m, in \u001b[0;36m_FunctionDecorator.__call__\u001b[0;34m(self, function, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_call()\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    168\u001b[0m   metadata\u001b[38;5;241m.\u001b[39mmark_error()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/tensorflow_datasets/core/load.py:212\u001b[0m, in \u001b[0;36mbuilder\u001b[0;34m(name, try_gcs, **builder_kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m:\n\u001b[1;32m    211\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m py_utils\u001b[38;5;241m.\u001b[39mtry_reraise(prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFailed to construct dataset \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbuilder_kwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pytype: disable=not-instantiable\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m# If neither the code nor the files are found, raise DatasetNotFoundError\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m not_found_error\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/tensorflow_datasets/core/logging/__init__.py:286\u001b[0m, in \u001b[0;36mbuilder_init.<locals>.decorator\u001b[0;34m(function, dsbuilder, args, kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m _thread_id_to_builder_init_count[metadata\u001b[38;5;241m.\u001b[39mthread_id] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 286\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    288\u001b[0m   metadata\u001b[38;5;241m.\u001b[39mmark_error()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/tensorflow_datasets/core/dataset_builder.py:1319\u001b[0m, in \u001b[0;36mFileReaderBuilder.__init__\u001b[0;34m(self, file_format, **kwargs)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[38;5;129m@tfds_logging\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder_init()\n\u001b[1;32m   1303\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m   1304\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1307\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1308\u001b[0m ):\n\u001b[1;32m   1309\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Initializes an instance of FileReaderBuilder.\u001b[39;00m\n\u001b[1;32m   1310\u001b[0m \n\u001b[1;32m   1311\u001b[0m \u001b[38;5;124;03m  Callers must pass arguments as keyword arguments.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1317\u001b[0m \u001b[38;5;124;03m    **kwargs: Arguments passed to `DatasetBuilder`.\u001b[39;00m\n\u001b[1;32m   1318\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1319\u001b[0m   \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1320\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mset_file_format(file_format)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/tensorflow_datasets/core/logging/__init__.py:286\u001b[0m, in \u001b[0;36mbuilder_init.<locals>.decorator\u001b[0;34m(function, dsbuilder, args, kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m _thread_id_to_builder_init_count[metadata\u001b[38;5;241m.\u001b[39mthread_id] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 286\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    288\u001b[0m   metadata\u001b[38;5;241m.\u001b[39mmark_error()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/tensorflow_datasets/core/dataset_builder.py:287\u001b[0m, in \u001b[0;36mDatasetBuilder.__init__\u001b[0;34m(self, data_dir, config, version)\u001b[0m\n\u001b[1;32m    285\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mread_from_directory(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_dir)\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Use the code version (do not restore data)\u001b[39;00m\n\u001b[0;32m--> 287\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize_from_bucket\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/tensorflow_datasets/core/dataset_info.py:784\u001b[0m, in \u001b[0;36mDatasetInfo.initialize_from_bucket\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;66;03m# In order to support Colab, we use the HTTP GCS API to access the metadata\u001b[39;00m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;66;03m# files. They are copied locally and then loaded.\u001b[39;00m\n\u001b[1;32m    783\u001b[0m tmp_dir \u001b[38;5;241m=\u001b[39m epath\u001b[38;5;241m.\u001b[39mPath(tempfile\u001b[38;5;241m.\u001b[39mmkdtemp(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtfds\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m--> 784\u001b[0m data_files \u001b[38;5;241m=\u001b[39m \u001b[43mgcs_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgcs_dataset_info_files\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data_files:\n\u001b[1;32m    786\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/tensorflow_datasets/core/utils/gcs_utils.py:100\u001b[0m, in \u001b[0;36mgcs_dataset_info_files\u001b[0;34m(dataset_name)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgcs_dataset_info_files\u001b[39m(dataset_name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[List[epath\u001b[38;5;241m.\u001b[39mPath]]:\n\u001b[1;32m     99\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Return paths to the dataset info files of the given dataset in gs://tfds-data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m   path \u001b[38;5;241m=\u001b[39m \u001b[43mgcs_dataset_info_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/tensorflow_datasets/core/utils/gcs_utils.py:93\u001b[0m, in \u001b[0;36mgcs_dataset_info_path\u001b[0;34m(dataset_name)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return paths to the dataset info files of the given dataset in gs://tfds-data.\"\"\"\u001b[39;00m\n\u001b[1;32m     92\u001b[0m path \u001b[38;5;241m=\u001b[39m gcs_path(posixpath\u001b[38;5;241m.\u001b[39mjoin(GCS_DATASET_INFO_DIR, dataset_name))\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_gcs_disabled \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mexists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     94\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m path\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/tensorflow_datasets/core/utils/gcs_utils.py:76\u001b[0m, in \u001b[0;36mexists\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Checks if path exists. Returns False if issues occur connecting to GCS.\"\"\"\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexists\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m gcs_unavailable_exceptions():  \u001b[38;5;66;03m# pylint: disable=catching-non-exception\u001b[39;00m\n\u001b[1;32m     78\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/etils/epath/gpath.py:144\u001b[0m, in \u001b[0;36m_GPath.exists\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexists\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m    143\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Returns True if self exists.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 144\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexists\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_path_str\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/etils/epath/backend.py:225\u001b[0m, in \u001b[0;36m_TfBackend.exists\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexists\u001b[39m(\u001b[38;5;28mself\u001b[39m, path: PathLike) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m--> 225\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/tensorflow/python/lib/io/file_io.py:290\u001b[0m, in \u001b[0;36mfile_exists_v2\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Determines whether a path exists or not.\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \n\u001b[1;32m    253\u001b[0m \u001b[38;5;124;03m>>> with open(\"/tmp/x\", \"w\") as f:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;124;03m  errors.OpError: Propagates any errors reported by the FileSystem API.\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 290\u001b[0m   \u001b[43m_pywrap_file_io\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFileExists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath_to_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mNotFoundError:\n\u001b[1;32m    292\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to construct dataset imagenet2012: pybind11::error_already_set: MISMATCH of original and normalized active exception types: ORIGINAL AbortedError REPLACED BY KeyboardInterrupt: <EMPTY MESSAGE>\n\nAt:\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/tensorflow/python/framework/errors_impl.py(412): __init__\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/tensorflow/python/lib/io/file_io.py(290): file_exists_v2\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/etils/epath/backend.py(225): exists\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/etils/epath/gpath.py(144): exists\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/tensorflow_datasets/core/utils/gcs_utils.py(76): exists\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/tensorflow_datasets/core/utils/gcs_utils.py(93): gcs_dataset_info_path\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/tensorflow_datasets/core/utils/gcs_utils.py(100): gcs_dataset_info_files\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/tensorflow_datasets/core/dataset_info.py(784): initialize_from_bucket\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/tensorflow_datasets/core/dataset_builder.py(287): __init__\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/tensorflow_datasets/core/logging/__init__.py(286): decorator\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/tensorflow_datasets/core/dataset_builder.py(1319): __init__\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/tensorflow_datasets/core/logging/__init__.py(286): decorator\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/tensorflow_datasets/core/load.py(212): builder\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/tensorflow_datasets/core/logging/__init__.py(166): __call__\n  /Users/yangyw/miniconda3/lib/python3.10/contextlib.py(79): inner\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/tensorflow_datasets/core/load.py(488): _fetch_builder\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/tensorflow_datasets/core/load.py(633): load\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/tensorflow_datasets/core/logging/__init__.py(166): __call__\n  /var/folders/pr/f8n46jm50p10n21j0n65__400000gn/T/ipykernel_7677/388126983.py(45): __init__\n  /var/folders/pr/f8n46jm50p10n21j0n65__400000gn/T/ipykernel_7677/3716585327.py(6): <module>\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py(3526): run_code\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py(3466): run_ast_nodes\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py(3284): run_cell_async\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/IPython/core/async_helpers.py(129): _pseudo_sync_runner\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py(3079): _run_cell\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py(3024): run_cell\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/ipykernel/zmqshell.py(546): run_cell\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/ipykernel/ipkernel.py(422): do_execute\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/ipykernel/kernelbase.py(740): execute_request\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/ipykernel/kernelbase.py(412): dispatch_shell\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/ipykernel/kernelbase.py(505): process_one\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/ipykernel/kernelbase.py(516): dispatch_queue\n  /Users/yangyw/miniconda3/lib/python3.10/asyncio/events.py(80): _run\n  /Users/yangyw/miniconda3/lib/python3.10/asyncio/base_events.py(1899): _run_once\n  /Users/yangyw/miniconda3/lib/python3.10/asyncio/base_events.py(603): run_forever\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/tornado/platform/asyncio.py(215): start\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/ipykernel/kernelapp.py(736): start\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/traitlets/config/application.py(1043): launch_instance\n  /Users/yangyw/miniconda3/lib/python3.10/site-packages/ipykernel_launcher.py(17): <module>\n  /Users/yangyw/miniconda3/lib/python3.10/runpy.py(86): _run_code\n  /Users/yangyw/miniconda3/lib/python3.10/runpy.py(196): _run_module_as_main\n"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "\n",
    "# Init Data Loaders\n",
    "train_data_loader = ImageNetDataLoader(\n",
    "        source_data_dir = \"/mnt/data/pycodes/Dataset/imagenet2012\",\n",
    "        dest_data_dir = \"/home/ani/Documents/datasets/imagenet\",\n",
    "        split = \"train\",\n",
    "        image_dims = (224, 224),\n",
    ")\n",
    "\n",
    "val_data_loader = ImageNetDataLoader(\n",
    "        source_data_dir = \"/mnt/data/pycodes/Dataset/imagenet2012\",\n",
    "        dest_data_dir = \"/home/ani/Documents/datasets/imagenet\",\n",
    "        split = \"validation\",\n",
    "        image_dims = (224, 224),\n",
    ")\n",
    "\n",
    "train_generator = train_data_loader.dataset_generator(batch_size=BATCH_SIZE, augment=False)\n",
    "val_generator = val_data_loader.dataset_generator(batch_size=BATCH_SIZE, augment=False)\n",
    "\n",
    "train_steps = train_data_loader.get_num_steps()\n",
    "val_steps = val_data_loader.get_num_steps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013ae3ac-6592-4474-b1ec-8b0c3d5fe88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "EPOCHS = 200\n",
    "\n",
    "# Compile & Train\n",
    "alexnet.compile(\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.SGD(\n",
    "        learning_rate=0.01, momentum=0.9, nesterov=False, name='SGD'\n",
    "    ),\n",
    "    metrics=metrics,\n",
    ")\n",
    "\n",
    "history = alexnet.fit(\n",
    "    epochs=EPOCHS,\n",
    "    x=train_generator,\n",
    "    steps_per_epoch=train_steps,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=val_steps,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2cf99588-5603-4276-931e-0620673c8d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = tf.one_hot(indices=5, depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3d55f720-817e-4317-9ba6-f8c3223766da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efbb4cd-06f7-46a4-9b23-17c381108023",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
