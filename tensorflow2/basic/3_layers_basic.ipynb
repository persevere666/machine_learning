{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.keras.layers.Embedding\n",
    "tf.keras.layers.Embedding(\n",
    "    input_dim,\n",
    "    output_dim,\n",
    "    embeddings_initializer='uniform',\n",
    "    embeddings_regularizer=None,\n",
    "    embeddings_constraint=None,\n",
    "    mask_zero=False,\n",
    "    lora_rank=None,\n",
    "    **kwargs\n",
    ")\n",
    "\n",
    "Args:\n",
    "\n",
    "        input_dim: Integer. Size of the vocabulary,\n",
    "            i.e. maximum integer index + 1.\n",
    "\n",
    "        output_dim: Integer. Dimension of the dense embedding.\n",
    "\n",
    "        embeddings_initializer: Initializer for the `embeddings`\n",
    "            matrix (see `keras.initializers`).\n",
    "\n",
    "        embeddings_regularizer: Regularizer function applied to\n",
    "            the `embeddings` matrix (see `keras.regularizers`).\n",
    "\n",
    "        embeddings_constraint: Constraint function applied to\n",
    "            the `embeddings` matrix (see `keras.constraints`).\n",
    "\n",
    "        mask_zero: Boolean, whether or not the input value 0 is a special\n",
    "            \"padding\" value that should be masked out.\n",
    "            This is useful when using recurrent layers which\n",
    "            may take variable length input. If this is `True`,\n",
    "            then all subsequent layers in the model need\n",
    "            to support masking or an exception will be raised.\n",
    "            If mask_zero is set to True, as a consequence,\n",
    "            index 0 cannot be used in the vocabulary (input_dim should\n",
    "            equal size of vocabulary + 1).\n",
    "            \n",
    "        lora_rank: Optional integer. If set, the layer's forward pass\n",
    "            will implement LoRA (Low-Rank Adaptation)\n",
    "            with the provided rank. LoRA sets the layer's embeddings\n",
    "            matrix to non-trainable and replaces it with a delta over the\n",
    "            original matrix, obtained via multiplying two lower-rank\n",
    "            trainable matrices. This can be useful to reduce the\n",
    "            computation cost of fine-tuning large embedding layers.\n",
    "            You can also enable LoRA on an existing\n",
    "            `Embedding` layer by calling `layer.enable_lora(rank)`.\n",
    "\n",
    "Input shape:\n",
    "\n",
    "    2D tensor with shape: `(batch_size, input_length)`.\n",
    "\n",
    "Output shape:\n",
    "\n",
    "    3D tensor with shape: `(batch_size, input_length, output_dim)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step\n",
      "(2, 10, 32)\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(1000,32))\n",
    "model.compile(\"rmsprop\",\"mse\")\n",
    "x = np.random.randint(1000,size=(2,10))\n",
    "y = model.predict(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.keras.layers.Dense\n",
    "    tf.keras.layers.Dense(\n",
    "        units,\n",
    "        activation=None,\n",
    "        use_bias=True,\n",
    "        kernel_initializer='glorot_uniform',\n",
    "        bias_initializer='zeros',\n",
    "        kernel_regularizer=None,\n",
    "        bias_regularizer=None,\n",
    "        activity_regularizer=None,\n",
    "        kernel_constraint=None,\n",
    "        bias_constraint=None,\n",
    "        lora_rank=None,\n",
    "        **kwargs\n",
    "    )\n",
    "    `Dense` implements the operation:\n",
    "        `output = activation(dot(input, kernel) + bias)`\n",
    "        where `activation` is the element-wise activation function\n",
    "        passed as the `activation` argument, `kernel` is a weights matrix\n",
    "        created by the layer, and `bias` is a bias vector created by the layer\n",
    "        (only applicable if `use_bias` is `True`).\n",
    "\n",
    "        Note: If the input to the layer has a rank greater than 2, `Dense`\n",
    "        computes the dot product between the `inputs` and the `kernel` along the\n",
    "        last axis of the `inputs` and axis 0 of the `kernel` (using `tf.tensordot`).\n",
    "        For example, if input has dimensions `(batch_size, d0, d1)`, then we create\n",
    "        a `kernel` with shape `(d1, units)`, and the `kernel` operates along axis 2\n",
    "        of the `input`, on every sub-tensor of shape `(1, 1, d1)` (there are\n",
    "        `batch_size * d0` such sub-tensors). The output in this case will have\n",
    "        shape `(batch_size, d0, units)`.\n",
    "\n",
    "    Args:\n",
    "        units: Positive integer, dimensionality of the output space.\n",
    "        activation: Activation function to use.\n",
    "            If you don't specify anything, no activation is applied\n",
    "            (ie. \"linear\" activation: `a(x) = x`).\n",
    "        use_bias: Boolean, whether the layer uses a bias vector.\n",
    "        kernel_initializer: Initializer for the `kernel` weights matrix.\n",
    "        bias_initializer: Initializer for the bias vector.\n",
    "        kernel_regularizer: Regularizer function applied to\n",
    "            the `kernel` weights matrix.\n",
    "        bias_regularizer: Regularizer function applied to the bias vector.\n",
    "        activity_regularizer: Regularizer function applied to\n",
    "            the output of the layer (its \"activation\").\n",
    "        kernel_constraint: Constraint function applied to\n",
    "            the `kernel` weights matrix.\n",
    "        bias_constraint: Constraint function applied to the bias vector.\n",
    "        lora_rank: Optional integer. If set, the layer's forward pass\n",
    "            will implement LoRA (Low-Rank Adaptation)\n",
    "            with the provided rank. LoRA sets the layer's kernel\n",
    "            to non-trainable and replaces it with a delta over the\n",
    "            original kernel, obtained via multiplying two lower-rank\n",
    "            trainable matrices. This can be useful to reduce the\n",
    "            computation cost of fine-tuning large dense layers.\n",
    "            You can also enable LoRA on an existing\n",
    "            `Dense` layer by calling `layer.enable_lora(rank)`.\n",
    "\n",
    "    Input shape:\n",
    "        N-D tensor with shape: `(batch_size, ..., input_dim)`.\n",
    "        The most common situation would be\n",
    "        a 2D input with shape `(batch_size, input_dim)`.\n",
    "\n",
    "    Output shape:\n",
    "        N-D tensor with shape: `(batch_size, ..., units)`.\n",
    "        For instance, for a 2D input with shape `(batch_size, input_dim)`,\n",
    "        the output would have shape `(batch_size, units)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[-0.64172983, -0.03486204,  0.34107637, -0.66875577],\n",
      "       [ 0.28248692,  0.64448667, -0.7345898 ,  0.55690575]],\n",
      "      dtype=float32)\n",
      "array([0., 0., 0., 0.], dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3, 4), dtype=float32, numpy=\n",
       "array([[[ 0.28248692,  0.64448667, -0.7345898 ,  0.55690575],\n",
       "        [-0.43599892,  1.8637359 , -1.5216167 ,  0.3332057 ],\n",
       "        [-1.1544847 ,  3.0829852 , -2.3086436 ,  0.10950565]],\n",
       "\n",
       "       [[-1.8729706 ,  4.3022346 , -3.0956707 , -0.11419439],\n",
       "        [-2.5914564 ,  5.5214834 , -3.882697  , -0.33789444],\n",
       "        [-3.3099422 ,  6.740733  , -4.6697245 , -0.5615945 ]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.arange(12).reshape(2,3,2)\n",
    "dense = tf.keras.layers.Dense(units=4)\n",
    "y = dense(x)\n",
    "weights, bias = dense.get_weights()\n",
    "tf.print(weights)\n",
    "tf.print(bias)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.28248692,  0.64448667, -0.73458982,  0.55690575],\n",
       "        [-0.43599892,  1.86373591, -1.5216167 ,  0.3332057 ],\n",
       "        [-1.15448475,  3.08298516, -2.30864358,  0.10950565]],\n",
       "\n",
       "       [[-1.87297058,  4.30223441, -3.09567046, -0.11419439],\n",
       "        [-2.59145641,  5.52148366, -3.88269734, -0.33789444],\n",
       "        [-3.30994225,  6.74073291, -4.66972423, -0.56159449]]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x @ weights + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3, 4), dtype=float32, numpy=\n",
       "array([[[ 0.28248692,  0.64448667, -0.7345898 ,  0.55690575],\n",
       "        [-0.43599892,  1.8637359 , -1.5216167 ,  0.3332057 ],\n",
       "        [-1.1544847 ,  3.0829852 , -2.3086436 ,  0.10950565]],\n",
       "\n",
       "       [[-1.8729706 ,  4.3022346 , -3.0956707 , -0.11419439],\n",
       "        [-2.5914564 ,  5.5214834 , -3.882697  , -0.33789444],\n",
       "        [-3.3099422 ,  6.740733  , -4.6697245 , -0.5615945 ]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.matmul(tf.convert_to_tensor(x,dtype=tf.float32),tf.convert_to_tensor(weights,dtype=tf.float32))+ tf.convert_to_tensor(bias,dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.keras.layers.Activation\n",
    "Inherits From: Layer, Operation\n",
    "\n",
    "tf.keras.layers.Activation(\n",
    "    activation, **kwargs\n",
    ")\n",
    "\n",
    "Args\n",
    "\n",
    "activation\tActivation function. It could be a callable, or the name of an activation from the keras.activations namespace.\n",
    "\n",
    "**kwargs\tBase layer keyword arguments, such as name and dtype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Activation name=activation, built=False>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = tf.keras.layers.Activation('relu')\n",
    "layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 0., 0., 2.], dtype=float32)>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.constant([-3.0, -1.0, 0.0, 2.0], dtype=tf.float32)\n",
    "layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=float32, numpy=array([-0.95021296, -0.63212055,  0.        ,  2.        ], dtype=float32)>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = tf.keras.layers.Activation(tf.keras.activations.elu)\n",
    "layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=float32, numpy=array([-0.95021296, -0.63212055,  0.        ,  2.        ], dtype=float32)>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.where(x<0, tf.math.exp(x)-1, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.keras.layers.Dropout\n",
    "\n",
    "tf.keras.layers.Dropout(\n",
    "    rate, noise_shape=None, seed=None, **kwargs\n",
    ")\n",
    "\n",
    "    The `Dropout` layer randomly sets input units to 0 with a frequency of\n",
    "    `rate` at each step during training time, which helps prevent overfitting.\n",
    "    Inputs not set to 0 are scaled up by `1 / (1 - rate)` such that the sum over\n",
    "    all inputs is unchanged.\n",
    "\n",
    "    Note that the `Dropout` layer only applies when `training` is set to `True`\n",
    "    in `call()`, such that no values are dropped during inference.\n",
    "    When using `model.fit`, `training` will be appropriately set to `True`\n",
    "    automatically. In other contexts, you can set the argument explicitly\n",
    "    to `True` when calling the layer.\n",
    "\n",
    "    (This is in contrast to setting `trainable=False` for a `Dropout` layer.\n",
    "    `trainable` does not affect the layer's behavior, as `Dropout` does\n",
    "    not have any variables/weights that can be frozen during training.)\n",
    "\n",
    "    Args:\n",
    "        rate: Float between 0 and 1. Fraction of the input units to drop.\n",
    "        noise_shape: 1D integer tensor representing the shape of the\n",
    "            binary dropout mask that will be multiplied with the input.\n",
    "            For instance, if your inputs have shape\n",
    "            `(batch_size, timesteps, features)` and\n",
    "            you want the dropout mask to be the same for all timesteps,\n",
    "            you can use `noise_shape=(batch_size, 1, features)`.\n",
    "        seed: A Python integer to use as random seed.\n",
    "\n",
    "    Call arguments:\n",
    "        inputs: Input tensor (of any rank).\n",
    "        training: Python boolean indicating whether the layer should behave in\n",
    "            training mode (adding dropout) or in inference mode (doing nothing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=2\n",
    "time_step=3\n",
    "num_feature=5\n",
    "input = np.random.random((batch_size,time_step,num_feature))\n",
    "dp1 = tf.keras.layers.Dropout(0.2)\n",
    "dp2 = tf.keras.layers.Dropout(0.2, noise_shape=[None,1,num_feature])\n",
    "y1 = dp1(input, training=True)\n",
    "y2 = dp2(input, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>> 0\n",
      "input\n",
      "[[0.96275613 0.32299979 0.25035313 0.35506693 0.84324772]\n",
      " [0.80321076 0.06097186 0.06952449 0.32996631 0.71498446]\n",
      " [0.80810178 0.39748406 0.40057398 0.46095772 0.59467154]]\n",
      "y1\n",
      "tf.Tensor(\n",
      "[[1.2034452  0.4037497  0.         0.44383365 1.0540596 ]\n",
      " [1.0040134  0.07621483 0.0869056  0.41245788 0.8937306 ]\n",
      " [1.0101272  0.49685508 0.50071746 0.57619715 0.7433394 ]], shape=(3, 5), dtype=float32)\n",
      "y2\n",
      "tf.Tensor(\n",
      "[[1.2034452  0.4037497  0.3129414  0.44383365 0.        ]\n",
      " [1.0040134  0.07621483 0.0869056  0.41245788 0.        ]\n",
      " [1.0101272  0.49685508 0.50071746 0.57619715 0.        ]], shape=(3, 5), dtype=float32)\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>> 1\n",
      "input\n",
      "[[0.01848825 0.13996816 0.06732699 0.60518559 0.92230018]\n",
      " [0.88649941 0.3624613  0.17512831 0.32605931 0.63917347]\n",
      " [0.21375652 0.88997229 0.5282882  0.68848123 0.40353503]]\n",
      "y1\n",
      "tf.Tensor(\n",
      "[[0.02311032 0.         0.         0.         1.1528752 ]\n",
      " [1.1081243  0.45307663 0.2189104  0.40757415 0.7989668 ]\n",
      " [0.26719564 1.1124654  0.6603602  0.86060154 0.5044188 ]], shape=(3, 5), dtype=float32)\n",
      "y2\n",
      "tf.Tensor(\n",
      "[[0.02311032 0.17496021 0.08415873 0.75648195 1.1528752 ]\n",
      " [1.1081243  0.45307663 0.2189104  0.40757415 0.7989668 ]\n",
      " [0.26719564 1.1124654  0.6603602  0.86060154 0.5044188 ]], shape=(3, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for i in range(batch_size):\n",
    "    print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>\", i)\n",
    "    print(\"input\")\n",
    "    print(input[i])\n",
    "    print(\"y1\")\n",
    "    print(y1[i])\n",
    "    print(\"y2\")\n",
    "    print(y2[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.keras.layers.AlphaDropout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.keras.layers.SpatialDropout2D\n",
    "AlphaDropout是一个更加强大的Dropout，它强大在两个地方：\n",
    "\n",
    "1. 均值和方差不变 （普通的Dropout在Dropout之后，可能这一层激活值的分布就发生变化了，但AlphaDropout不会）\n",
    "\n",
    "2. 归一化性质也不变（因为均值和方差不变，所以归一化性质也不变，有了这个性质之后，这个Dropout就可以和批归一化、selu在一块使用，因为它不会导致分布发生变化）\n",
    "\n",
    "tf.keras.layers.SpatialDropout2D(\n",
    "    rate, data_format=None, seed=None, name=None, dtype=None\n",
    ")\n",
    "\n",
    "This version performs the same function as Dropout, however, it drops\n",
    "    entire 2D feature maps instead of individual elements. If adjacent pixels\n",
    "    within feature maps are strongly correlated (as is normally the case in\n",
    "    early convolution layers) then regular dropout will not regularize the\n",
    "    activations and will otherwise just result in an effective learning rate\n",
    "    decrease. In this case, `SpatialDropout2D` will help promote independence\n",
    "    between feature maps and should be used instead.\n",
    "\n",
    "    Args:\n",
    "        rate: Float between 0 and 1. Fraction of the input units to drop.\n",
    "        data_format: `\"channels_first\"` or `\"channels_last\"`.\n",
    "            In `\"channels_first\"` mode, the channels dimension (the depth)\n",
    "            is at index 1, in `\"channels_last\"` mode is it at index 3.\n",
    "            It defaults to the `image_data_format` value found in your\n",
    "            Keras config file at `~/.keras/keras.json`.\n",
    "            If you never set it, then it will be `\"channels_last\"`.\n",
    "\n",
    "    Call arguments:\n",
    "        inputs: A 4D tensor.\n",
    "        training: Python boolean indicating whether the layer\n",
    "            should behave in training mode (applying dropout)\n",
    "            or in inference mode (pass-through).\n",
    "\n",
    "    Input shape:\n",
    "        4D tensor with shape: `(samples, channels, rows, cols)` if\n",
    "            data_format='channels_first'\n",
    "        or 4D tensor with shape: `(samples, rows, cols, channels)` if\n",
    "            data_format='channels_last'.\n",
    "\n",
    "    Output shape: Same as input.\n",
    "\n",
    "    Reference:\n",
    "\n",
    "    - [Tompson et al., 2014](https://arxiv.org/abs/1411.4280)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[0.616599381 1.60801017 1.32021952]\n",
      "   [0.570951164 0.672822714 1.04828417]]\n",
      "\n",
      "  [[0.0512940101 1.58575416 0.90588516]\n",
      "   [1.32246041 0.097947225 0.280347347]]]]\n"
     ]
    }
   ],
   "source": [
    "x = np.random.random((1,2,2,3))\n",
    "x = tf.constant(x, dtype=tf.float32)\n",
    "sdropout = tf.keras.layers.SpatialDropout2D(0.4)\n",
    "y = sdropout(x, training=True)\n",
    "tf.print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.keras.layers.SpatialDropout1D\n",
    "Input shape\n",
    "\n",
    "3D tensor with shape: (samples, timesteps, channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.keras.layers.SpatialDropout3D\n",
    "Input shape\n",
    "\n",
    "5D tensor with shape: (samples, channels, dim1, dim2, dim3) if data_format='channels_first' or 5D tensor with shape: (samples, dim1, dim2, dim3, channels) if data_format='channels_last'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.keras.layers.Flatten\n",
    "\n",
    "* tf.keras.layers.Flatten(\n",
    "    data_format=None, **kwargs\n",
    ")\n",
    "\n",
    "- Flattens the input. Does not affect the batch size.\n",
    "\n",
    "    Note: If inputs are shaped `(batch,)` without a feature axis, then\n",
    "    flattening adds an extra channel dimension and output shape is `(batch, 1)`.\n",
    "\n",
    "    Args:\n",
    "    \n",
    "        data_format: A string, one of `\"channels_last\"` (default) or\n",
    "            `\"channels_first\"`. The ordering of the dimensions in the inputs.\n",
    "            `\"channels_last\"` corresponds to inputs with shape\n",
    "            `(batch, ..., channels)` while `\"channels_first\"` corresponds to\n",
    "            inputs with shape `(batch, channels, ...)`.\n",
    "            When unspecified, uses `image_data_format` value found in your Keras\n",
    "            config file at `~/.keras/keras.json` (if exists). Defaults to\n",
    "            `\"channels_last\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor shape=(None, 640), dtype=float32, sparse=None, name=keras_tensor_9>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.keras.Input(shape=(10,64))\n",
    "y = tf.keras.layers.Flatten()(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.keras.layers.Reshape\n",
    "* tf.keras.layers.Reshape(\n",
    "    target_shape, **kwargs\n",
    ")\n",
    "\n",
    "Layer that reshapes inputs into the given shape.\n",
    "\n",
    "* Args:\n",
    "        target_shape: Target shape. Tuple of integers, does not include the\n",
    "            samples dimension (batch size).\n",
    "\n",
    "* Input shape:\n",
    "        Arbitrary, although all dimensions in the input shape must be\n",
    "        known/fixed. Use the keyword argument `input_shape` (tuple of integers,\n",
    "        does not include the samples/batch size axis) when using this layer as\n",
    "        the first layer in a model.\n",
    "\n",
    "* Output shape:\n",
    "    `(batch_size, *target_shape)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3, 4), dtype=float32, numpy=\n",
       "array([[[0.6452224 , 0.06654943, 0.9695012 , 0.44724554],\n",
       "        [0.2576106 , 0.6819731 , 0.8684523 , 0.6802403 ],\n",
       "        [0.20798987, 0.6424835 , 0.6363279 , 0.2933764 ]],\n",
       "\n",
       "       [[0.9978972 , 0.761238  , 0.3668906 , 0.11984321],\n",
       "        [0.5717654 , 0.02247417, 0.47233447, 0.7455886 ],\n",
       "        [0.8301009 , 0.90731865, 0.07383701, 0.16560638]]], dtype=float32)>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.random((2,12))\n",
    "x = tf.constant(x, dtype=tf.float32)\n",
    "y = tf.keras.layers.Reshape(target_shape=(3,4))(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 3, 2, 2])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = tf.keras.layers.Reshape(target_shape=(-1,2,2))(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 2, 2, 3)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.keras.Input(shape=(12,))\n",
    "y = tf.keras.layers.Reshape(target_shape=(-1,2,3))(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.keras.layers.Concatenate\n",
    "    It takes as input a list of tensors, all of the same shape except\n",
    "    for the concatenation axis, and returns a single tensor that is the\n",
    "    concatenation of all inputs.\n",
    "\n",
    "* tf.keras.layers.Concatenate(axis=-1, **kwargs)\n",
    "* Args:\n",
    "        axis: Axis along which to concatenate.\n",
    "        **kwargs: Standard layer keyword arguments.\n",
    "\n",
    "* Returns:\n",
    "        A tensor, the concatenation of the inputs alongside axis `axis`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 3, 5])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = np.arange(20).reshape(2,2,5)\n",
    "x2 = np.arange(20,30).reshape(2,1,5)\n",
    "y = tf.keras.layers.Concatenate(axis=1)([x1,x2])\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorShape([5, 8])\n",
      "TensorShape([5, 8])\n",
      "TensorShape([5, 16])\n"
     ]
    }
   ],
   "source": [
    "x1 = tf.keras.layers.Dense(8)(np.arange(10).reshape(5,2))\n",
    "x2 = tf.keras.layers.Dense(8)(np.arange(10,20).reshape(5,2))\n",
    "tf.print(x1.shape)\n",
    "tf.print(x2.shape)\n",
    "y=tf.keras.layers.Concatenate()([x1,x2])\n",
    "tf.print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.keras.layers.Add\n",
    "tf.keras.layers.Add(\n",
    "    **kwargs\n",
    ")\n",
    "    \n",
    "    It takes as input a list of tensors, all of the same shape,\n",
    "    and returns a single tensor (also of the same shape).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3, 4), dtype=float32, numpy=\n",
       "array([[[0.48554325, 1.227601  , 0.8126565 , 0.53688073],\n",
       "        [0.68666613, 1.6902375 , 1.0352478 , 0.57138455],\n",
       "        [1.0321999 , 0.501016  , 0.9576036 , 1.2120421 ]],\n",
       "\n",
       "       [[0.7342508 , 1.4534317 , 0.71955824, 1.7794999 ],\n",
       "        [1.6020393 , 1.3596292 , 0.6303549 , 0.39443696],\n",
       "        [0.53170365, 1.138582  , 1.7356129 , 0.96787584]]], dtype=float32)>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape=(2,3,4)\n",
    "x1 = np.random.random(input_shape)\n",
    "x2 = np.random.rand(*input_shape)\n",
    "y = tf.keras.layers.Add()([x1,x2])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.48554325, 1.22760105, 0.8126565 , 0.53688073],\n",
       "        [0.68666611, 1.69023744, 1.03524777, 0.57138452],\n",
       "        [1.03219991, 0.50101602, 0.95760354, 1.21204215]],\n",
       "\n",
       "       [[0.73425074, 1.45343176, 0.71955827, 1.77949984],\n",
       "        [1.60203927, 1.35962916, 0.63035487, 0.39443696],\n",
       "        [0.53170364, 1.13858196, 1.73561293, 0.96787586]]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 + x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.keras.layers.Subtract\n",
    "tf.keras.layers.Subtract(\n",
    "    **kwargs\n",
    ")\n",
    "\n",
    "    Performs elementwise subtraction.\n",
    "    It takes as input a list of tensors of size 2 both of the\n",
    "    same shape, and returns a single tensor (inputs[0] - inputs[1])\n",
    "    of same shape.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3, 4), dtype=float32, numpy=\n",
       "array([[[-0.05820923,  0.54523873, -0.33016336,  0.4021411 ],\n",
       "        [-0.19446963,  0.1893099 ,  0.76850337, -0.49797586],\n",
       "        [-0.6312529 , -0.3815776 ,  0.46261907,  0.01131099]],\n",
       "\n",
       "       [[-0.7241807 ,  0.44210625, -0.38760757, -0.01166928],\n",
       "        [-0.0849604 , -0.07201624, -0.31166935,  0.09301943],\n",
       "        [ 0.01433945,  0.09968823,  0.18223101, -0.8487245 ]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.layers.Subtract()([x1,x2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.keras.layers.Maximum\n",
    "\n",
    "tf.keras.layers.Maximum(\n",
    "    **kwargs\n",
    ")\n",
    "\n",
    "It takes as input a list of tensors, all of the same shape, and returns a single tensor (also of the same shape)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.05820921,  0.54523875, -0.33016335,  0.40214111],\n",
       "        [-0.19446962,  0.18930986,  0.76850337, -0.49797586],\n",
       "        [-0.63125289, -0.38157764,  0.46261907,  0.01131104]],\n",
       "\n",
       "       [[-0.72418069,  0.44210626, -0.38760759, -0.01166928],\n",
       "        [-0.0849604 , -0.07201623, -0.31166938,  0.09301944],\n",
       "        [ 0.01433943,  0.09968821,  0.18223097, -0.84872448]]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 - x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[[0.81355413, 0.82931134, 0.22891   , 0.15438937],\n",
      "        [0.46969721, 0.18594595, 0.37698281, 0.99312283],\n",
      "        [0.14015528, 0.33212982, 0.46019887, 0.93527131]],\n",
      "\n",
      "       [[0.4736411 , 0.13906825, 0.5870708 , 0.31000703],\n",
      "        [0.69630795, 0.51727792, 0.75252039, 0.4710736 ],\n",
      "        [0.68655642, 0.22620015, 0.85941165, 0.5590729 ]]])\n",
      "array([[[0.41427353, 0.4100347 , 0.03177998, 0.85128087],\n",
      "        [0.74203412, 0.62904349, 0.44725248, 0.74641753],\n",
      "        [0.39199419, 0.40299547, 0.65917749, 0.2838739 ]],\n",
      "\n",
      "       [[0.59430464, 0.94850065, 0.11541002, 0.06381379],\n",
      "        [0.15556203, 0.77173534, 0.01443398, 0.15971388],\n",
      "        [0.52261042, 0.54968582, 0.71589447, 0.8129841 ]]])\n",
      "[[[0.813554108 0.829311311 0.22891 0.851280868]\n",
      "  [0.742034137 0.629043519 0.447252482 0.993122816]\n",
      "  [0.391994178 0.402995467 0.659177482 0.935271323]]\n",
      "\n",
      " [[0.594304621 0.948500633 0.587070823 0.310007036]\n",
      "  [0.696307957 0.771735311 0.752520382 0.471073598]\n",
      "  [0.686556399 0.549685836 0.859411657 0.812984109]]]\n"
     ]
    }
   ],
   "source": [
    "input_shape = (2, 3, 4)\n",
    "x1 = np.random.rand(*input_shape)\n",
    "x2 = np.random.rand(*input_shape)\n",
    "y = tf.keras.layers.Maximum()([x1, x2])\n",
    "tf.print(x1)\n",
    "tf.print(x2)\n",
    "tf.print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3, 4), dtype=float64, numpy=\n",
       "array([[[0.81355413, 0.82931134, 0.22891   , 0.85128087],\n",
       "        [0.74203412, 0.62904349, 0.44725248, 0.99312283],\n",
       "        [0.39199419, 0.40299547, 0.65917749, 0.93527131]],\n",
       "\n",
       "       [[0.59430464, 0.94850065, 0.5870708 , 0.31000703],\n",
       "        [0.69630795, 0.77173534, 0.75252039, 0.4710736 ],\n",
       "        [0.68655642, 0.54968582, 0.85941165, 0.8129841 ]]])>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.where(x1>x2,x1,x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.keras.layers.Maximum"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
