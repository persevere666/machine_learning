{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5d0c1cc-d159-4095-a2a8-4a83a7ff4730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.99-cp310-cp310-macosx_10_9_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m944.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-22 15:19:27.820628: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "try:\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "# !pip install tensorflow_probability==0.8.0rc0 --upgrade\n",
    "!pip install sentencepiece\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de7f28e0-e566-40e9-a105-bd06e97c3d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p checkpoints\n",
    "!mkdir -p datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2729e602-fb77-4041-8243-e7646c19547f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow==2.13.0\n",
      "tensorflow-addons==0.21.0\n",
      "tensorflow-datasets==4.9.3\n",
      "tensorflow-estimator==2.13.0\n",
      "tensorflow-io-gcs-filesystem==0.34.0\n",
      "tensorflow-metadata==1.14.0\n",
      "/bin/bash: nvidia-smi: command not found\n"
     ]
    }
   ],
   "source": [
    "# print tensorflow versions\n",
    "!pip freeze | grep tensorflow\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a88bc6c-15e9-4405-878c-0bb811f80896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /var/folders/pr/f8n46jm50p10n21j0n65__400000gn/T/ipykernel_23086/577727938.py:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "is gpu available?:  False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 6935092614327954695\n",
       " xla_global_id: -1]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('is gpu available?: ', tf.test.is_gpu_available())\n",
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93dd01f9-de76-495c-9540-572f66ea7133",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddinglayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        # model hyper parameter variables\n",
    "        super(Embeddinglayer, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model)\n",
    "    \n",
    "    def call(self, sequences):\n",
    "        max_sequence_len = sequences.shape[1]\n",
    "        output = self.embedding(sequences) * tf.sqrt(tf.cast(self.d_model, dtype=tf.float32))\n",
    "        output += self.positional_encoding(max_sequence_len)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def positional_encoding(self, max_len):\n",
    "        pos = np.expand_dims(np.arange(0, max_len), axis=1)\n",
    "        index = np.expand_dims(np.arange(0, self.d_model), axis=0)\n",
    "        \n",
    "        pe = self.angle(pos, index)\n",
    "        \n",
    "        pe[:, 0::2] = np.sin(pe[:, 0::2])\n",
    "        pe[:, 1::2] = np.cos(pe[:, 1::2])        \n",
    "        \n",
    "        pe = np.expand_dims(pe, axis=0)\n",
    "        return tf.cast(pe, dtype=tf.float32)\n",
    "        \n",
    "    def angle(self, pos, index):\n",
    "        return pos / np.power(10000, (index - index % 2) / np.float32(self.d_model))\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97793d52-10b1-4a7a-8254-39b50f3a7cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_h):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.d_h = d_h\n",
    "        \n",
    "    def call(self, query, key, value, mask=None):\n",
    "        # query = (batch_size, attention_head_count, seq_length, d_h)\n",
    "        # matmul_q_and_transposed_k = (batch_size, attention_head_count, seq_length, seq_length)\n",
    "        matmul_q_and_transposed_k = tf.matmul(query, key, transpose_b=True)\n",
    "        \n",
    "        scale = tf.sqrt(tf.cast(self.d_h, dtype=tf.float32))\n",
    "        scaled_attention_score = matmul_q_and_transposed_k / scale\n",
    "        if mask is not None:\n",
    "            scaled_attention_score += (mask * -1e9)\n",
    "        \n",
    "        attention_weight = tf.nn.softmax(scaled_attention_score, axis=-1)\n",
    "        \n",
    "        return tf.matmul(attention_weight, value), attention_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ea5bf6-c1fa-4e0f-b592-a015ab752c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, attention_head_count, d_model, dropout_prob):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        # model hyper parameter variables\n",
    "        self.attention_head_count = attention_head_count\n",
    "        self.d_model = d_model\n",
    "        self.dropout_prob = dropout_prob\n",
    "\n",
    "        if d_model % attention_head_count != 0:\n",
    "            raise ValueError(\n",
    "                f\"d_model({d_model}) % attention_head_count({attention_head_count}) is not zero.\"\n",
    "                f\"d_model must be multiple of attention_head_count.\"\n",
    "            )\n",
    "        \n",
    "        self.d_h = d_model // attention_head_count\n",
    "        \n",
    "        self.w_query = tf.keras.layers.Dense(d_model)\n",
    "        self.w_key = tf.keras.layers.Dense(d_model)\n",
    "        self.w_value = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.scaled_dot_product = ScaledDotProductAttention(self.d_h)\n",
    "        \n",
    "        self.ff = tf.keras.layers.Dense(d_model)\n",
    "    \n",
    "    def call(self, query, key, value, mask=None):\n",
    "        # query=input\n",
    "        batch_size = tf.shape(query)[0]\n",
    "        # query_shape = (batch_size, sentene_length, d_model)\n",
    "        query = self.w_query(query)\n",
    "        \n",
    "        key = self.w_key(key)\n",
    "        \n",
    "        value = self.w_value(value)\n",
    "        # query_shape = (batch_size, attention_head_count, sentene_length, d_h)\n",
    "        query = self.split_head(query, batch_size)\n",
    "        key = self.split_head(key, batch_size)\n",
    "        value = self.split_head(value, batch_size)\n",
    "        \n",
    "        output, attention = self.scaled_dot_product(query, key, value, mask)\n",
    "        \n",
    "        output = self.concat_head(output, batch_size)\n",
    "        \n",
    "        return self.ff(output), attention\n",
    "        \n",
    "    \n",
    "    def split_head(self, tensor, batch_size):\n",
    "        # input tensor: (batch_size, seq_len, d_model)\n",
    "        return tf.transpose(\n",
    "            tf.reshape(\n",
    "                tensor, \n",
    "                (batch_size, -1, self.attention_head_count, self.d_h)\n",
    "                # tensor: (batch_size, seq_len_splited, attention_head_count, d_h)\n",
    "            ),\n",
    "            [0, 2, 1, 3]\n",
    "            # tensor: (batch_size, attention_head_count, seq_len_splited, d_h)\n",
    "        )\n",
    "    \n",
    "    def concat_head(self, tensor, batch_size):\n",
    "        return tf.reshape(\n",
    "            tf.transpose(tensor, [0, 2, 1, 3]), \n",
    "            (batch_size, -1, self.attention_head_count * self.d_h)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
